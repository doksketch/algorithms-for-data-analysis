{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Переход от МНК к методу максимального правдоподобия "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть в каждой точке пространства объектов $\\mathbb{X}$ задана вероятность того, что объект $x$ будет принадлежать к классу \"+1\" $P(y=1|x)$ (условная вероятность $y = 1$ при условии $x$). Она будет принимать значения от 0 до 1, и нам нужно каким-то образом ее предсказывать, но пока мы умеем только строить прогноз методами линейной регрессии с помощью некоего алгоритма $b(x)=\\left \\langle w,x_{i} \\right \\rangle$. У него есть проблема, связанная с тем, что скалярное произведение $\\left \\langle w,x_{i} \\right \\rangle$ не всегда возвращает значения в отрезке [0, 1]. Если полученные вещественные значения отнормировать по шкале [0, 1], то это уже не будет вероятностью. Также, если будет необходимо возвращать значения в отрезке [0, 1], то максимальная ошибка для MSE будет единица. Чтобы достичь такого условия, можно использовать некую функцию $\\sigma:\\mathbb{R} \\rightarrow [0,1]$, которая будет переводить полученное в скалярном произведении значение в вероятность, пределы которой будут лежать в промежутке от 0 до 1. В модели логистической регрессии в качестве такой функции берется сигмоида, которая имеет вид:\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + exp(-z)}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dots = np.linspace(-10, 10, 100)\n",
    "sigmoid_value = list(map(sigmoid, dots))\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('sigmoid(x)')\n",
    "plt.grid()\n",
    "plt.plot(dots, sigmoid_value);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одно из свойств сигмоиды - при нулевом аргументе она равна 0.5(50 процентов). Чем выше аргумент, тем сигмоида ближе к 1,\n",
    "чем ниже - тем ближе к 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При использовании такой функции $\\tilde{b}(x_{i}) = \\sigma(\\left \\langle w,x_{i} \\right \\rangle)$ получаем, что вероятность отнесения объекта к классу \"+1\" $P(y=1|x)$, которую для краткости обозначим $p_{+}$, будет равняться\n",
    "\n",
    "$$p_{+} = \\sigma(\\left \\langle w,x_{i} \\right \\rangle) = \\frac{1}{1 + exp(-\\left \\langle w,x_{i} \\right \\rangle)},$$\n",
    "\n",
    "Чем больше будет скалярное произведение $\\left \\langle w,x_{i} \\right \\rangle$, тем выше будет предсказанная вероятность."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы понять, как его интерпретировать, выведем его из формулы выше:\n",
    "\n",
    "$$\\left \\langle w,x_{i} \\right \\rangle = \\text{ln} \\frac{p_{+}}{1 - p_{+}}.$$\n",
    "\n",
    "Таким образом, скалярное произведение вектора весов на вектор признаков представляет собой логарифм отношения вероятностей того, что y = 1 к вероятности того, что y = -1. Выражение под логарифмом называется _риском_, а вместе с логарифмом это выражение называется _логитом_. Поэтому метод и называется логистической регрессией: мы приближаем логит линейной комбинацией признаков и весов. Т.е. для того, чтобы решить проблему нахождения скалярного произведения матрицы весов на матрицу признаков, можно решать задачу при помощи MSE и получившийся ответ применить к сигмоиде."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Метод максимального правдоподобия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее для обучения этой модели нам потребуется использовать _метод максимального правдоподобия_. Его сущность заколючается в выборе гипотезы, при которой вероятность получить имеющееся наблюдение максимальна.\n",
    "\n",
    "С точки зрения реализуемого алгоритма вероятность того, что в выборке встретится объект $x_{i}$ c классом $y_{i}$, равна\n",
    "\n",
    "$$P(y=y_{i}|x_{i}) = p_{+}^{[y_{i}=+1]}(1-p_{+})^{[y_{i}=-1]}.$$\n",
    "\n",
    "Исходя из этого, правдоподобие выборки (т.е. вероятность получить такую выборку с точки зрения алгоритма) будет равняться произведению вероятностей получения каждого имеющегося ответа:\n",
    "\n",
    "$$P(y|X) = L(X) = \\prod^{l}_{i=1} p_{+}^{[y_{i}=+1]}(1-p_{+})^{[y_{i}=-1]}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Логарифмическая функция потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Правдоподобие можно использовать как функционал для обучения алгоритма, однако, удобнее взять от него логарифм, так как в этом случае произведение превратится в сумму(по свойству логарифма), а сумму гораздо проще оптимизировать. Также, в отличие от рассмотренных ранее функций потерь, правдоподобие требуется максимизировать для обучения алгоритма, а не минимизировать. Поэтому для большего удобства перед правдоподобием ставят минус, поскольку функции потери в задачах регрессии принято минимизировать. В итоге получим:\n",
    "\n",
    "$$-\\text{ln}L(X) = -\\sum^{l}_{i=1}([y_{i} = +1] \\text{ln}p_{+}) + [y_{i} = -1]\\text{ln}(1 - p_{+}))).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](loggraph.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из графика видим, что логарифм от нуля не определён. Ноль под логарифмом может оказаться тогда, когда предсказанное значение \n",
    "класса прямо противоположно истинному, поскольку получим бесконечность и бесконечную ошибку. При использовании MSE ошибка может\n",
    "быть равна максимум единице. Для алгоритма это свойство выполянет функцию очень сильного \"штрафа\" и он стремится предсказывать\n",
    "значение вероятности(класса) точнее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы подставим в нее полученное ранее выражение для $p_{+}$ для сигмоиды, получим логарифмическую функцию потерь:\n",
    "\n",
    "$$-\\text{ln}L(X) = -\\sum^{l}_{i=1}([y_{i} = +1] \\text{ln}\\frac{1}{1 + exp(-\\left \\langle w,x_{i} \\right \\rangle)}) + [y_{i} = -1]\\text{ln}(1 - \\frac{1}{1 + exp(-\\left \\langle w,x_{i} \\right \\rangle)}))) =$$\n",
    "\n",
    "$$=-\\sum^{l}_{i=1} \\text{ln}(1 + exp(\\left \\langle w,x_{i} \\right \\rangle))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Оценка качества классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  <empty>   | $$y = +1$$ | $$y = -1$$ |\n",
    "--- | --- | ---\n",
    "| __$$a(x) = +1$$__  |   TP    |   FP   |\n",
    "| __$$a(x) = -1$$__ |   FN    |   TN   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cверху отложены истинные ответы, слева - ответы алгоритма. Когда алгоритм относит объект к классу \"+1\", говорят, что он _срабатывает_, а когда к \"-1\", - _пропускает_. Если алгоритм сработал (дал положительный ответ) и объект действительно относится к классу \"+1\", говорят, что имеет место верное срабатывание/верный положительный ответ (True Positive, TP), а если объект не относится к классу \"+1\", это ложное срабатывание (False Positive, FP). Если алгоритм пропускает объект, а его истинный класс \"+1\", это ложный пропуск/ложный негативные ответ (False Negative, FN), а если истинный класс объекта \"-1\", имеет место истинный пропуск (True Negative, TN). При такой классификации уже есть два вида ошибок - ложные срабатывания и ложные пропуски. По главной диагонали в матрице ошибок располагаются верные ответы, по побочной - неверные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy**`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наиболее очевидным и простым способом является расчет _доли правильных ответов_:\n",
    "\n",
    "$$accuracy(a,x) = \\frac{TP+TN}{TP+TN+FP+FN}.$$\n",
    "\n",
    "Эта метрика очень легко интерпретируется, однако, имеет определенные недостатки:\n",
    "\n",
    "- Она может неадекватно работать на несбалансированных выборках, в которых объектов одного класса намного больше остальных: например, если у нас имеется выборка с 950 объектами класса +1 и 50 класса -1, обыкновенная константная модель классификатора, которая на всех объектах отдает ответ +1, будет иметь долю правильных ответов 0,95, при этом сам классификатор является абсолютно бесполезным. Методом борьбы с этим заключается в введении коэффициента $q_{0}$, равного доле объектов самого большого класса. Доля правильных ответов для корректных алгоритмов должна лежать в промежутке $[q_{0}, 1]$\n",
    "\n",
    "- Она не учитывает \"цены ошибок\". В некоторых прикладных задачах ошибки разного рода могут иметь разную важность. Например, если говорить о кредитном скоринге, при постановке задачи необходимо определить, какая ошибка будет хуже: выдать кредит \"плохому\" клиенту или не выдать \"хорошему\". При этом используемая метрика качества должна учитывать цены разных ошибок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Точность и полнота**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точность (precision) представляет из себя долю истинных срабатываний от общего количества срабатываний. Она показывает, насколько можно доверять алгоритму классификации в случае срабатывания\n",
    "\n",
    "$$precision(a, X) = \\frac{TP}{TP+FP}.$$\n",
    "\n",
    "Полнота (recall) считается как доля объектов, истинно относящихся к классу \"+1\", которые алгоритм отнес к этому классу\n",
    "\n",
    "$$recall(a, X) = \\frac{TP}{TP+FN},$$\n",
    "\n",
    "здесь $TP+FN$ как раз будут вместе составлять весь список объектов класса \"+1\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы максимизировать precision, нужно чтобы было как можно меньше ошибок отнесения объекта к классу 1, когда в реальности\n",
    "это событие - 0. Этот вариант подходит тогда, когда цена ошибки большая. Максимизировать recall можно при помощи минимизации количества случаев отнесения \n",
    "события к 0, когда в реальности это событие - 1(FN). Эту метрику лучше использовать тогда, когда цена воздействия на событие маленькая."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, если в задаче кредитного скоринга банк ставит цель возврата 90% кредитов, задачей ставится максимизация полноты при условии точности не ниже 0.9. А если при распознавании спама стоит требование, например, распознавать 95% спам-писем, задача состоит в максимизации точности при условии полноты не ниже 0.95."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F1 мера(гармоническое среднее)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В некоторых случаях требуется максимизировать и полноту и точность. _F-мера_ - это среднее гармоническое между точностью и полнотой.\n",
    "\n",
    "$$F = \\frac{2 \\cdot precision \\cdot recall }{ presision + recall}.$$\n",
    "\n",
    "В отличие от, например, среднего арифметического, если хотя бы один из аргументов близок к нулю, то и среднее гармоническое будет близко к нулю."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROC - AUC**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Примеры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Подберите аргументы функции eval_model для логистической регрессии таким образом, чтобы log loss был минимальным.\n",
    "-Создайте функцию calc_pred_proba, возвращающую предсказанную вероятность класса 1 (на вход подаются W, который уже посчитан -функцией eval_model и X, на выходе - массив y_pred_proba).\n",
    "-Создайте функцию calc_pred, возвращающую предсказанный класс (на вход подаются W, который уже посчитан функцией eval_model и X, на выходе - массив y_pred).\n",
    "-Посчитайте Accuracy, матрицу ошибок, точность и полноту, а также F1 score.\n",
    "-Могла ли модель переобучиться? Почему?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Логистическая функция потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Проверка на ненулевое выражение под логарифмом\n",
    "def log_checking(y_pred):\n",
    "    assert y_pred != 0\n",
    "    assert y_pred != 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Подсчёт логистической функции потерь с проверкой на ненулевое выражение под логарифмом\n",
    "def calc_logloss(y, y_pred):\n",
    "    log_checking(y_pred)\n",
    "    err = - np.mean(y * np.log(y_pred) + (1.0 - y) * np.log(1.0 - y_pred))\n",
    "    return err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Минимизация log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    res = 1 / (1 + np.exp(-z))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Проходим по вариантам aplha и iterations\n",
    "def eval_logit(X, y, iterations=None, alpha=None):\n",
    "    np.random.seed(42)\n",
    "    W = np.random.randn(X.shape[0])\n",
    "    n = X.shape[1]\n",
    "    \n",
    "    for i in range(1, iterations+1):\n",
    "        for j in alpha:\n",
    "            z = np.dot(W, X)\n",
    "            y_pred = sigmoid(z)\n",
    "            err = calc_logloss(y, y_pred)\n",
    "            W -= alpha * (1/n * np.dot((y_pred - y), X.T))\n",
    "    \n",
    "    if i % (iterations / 10) == 0:\n",
    "        print(i, W, err)\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#W = eval_logit(X, y, iterations=[100, 300, 700], alpha=[1e-5, 1e-3, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Функция, рассчитывающая вероятность отнесения к классу 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pred_proba(W, X):\n",
    "    y_pred_proba = 1/(1 + np.exp(-(np.dot(X,W))))\n",
    "    return y_pred_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Функция, рассчитывающая вероятность отнесения к классу 0 или 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# За порог отнесения к тому или иному классу примем вероятность 0.5\n",
    "def calc_pred(W, X):\n",
    "    m = X.shape[1]\n",
    "    y_pred = np.zeros((1, m))\n",
    "    \n",
    "    y_pred_proba = calc_pred_proba(W, X)\n",
    "    \n",
    "    for i in range(y_pred_proba.shape[1]):\n",
    "        \n",
    "        if y_pred_proba[:, i] > 0.5:\n",
    "            y_pred[:, i] = 1\n",
    "        else:\n",
    "            y_pred[:, i] = 0\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. Расчёт Accuracy, матрицы ошибок, точности и полноты, F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Матрица ошибок 1 и 2 рода\n",
    "def matrix_errors(y, y_pred):\n",
    "    spam = zip(y, y_pred)\n",
    "    \n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    \n",
    "    for i in range(len(spam)):\n",
    "        if spam[i][0] == 1 and spam[i][1] == 1: #хорошего человека назвали хорошим\n",
    "            TP += 1\n",
    "        elif spam[i][0] < spam[i][1]: #плохого человека назвали хорошим\n",
    "            FP += 1\n",
    "        elif spam[i][0] < spam[i][1]: #хорошего человека назвали плохим\n",
    "            FN += 1\n",
    "        else:\n",
    "            TN += 1 #плохого человека назвали плохим\n",
    "    \n",
    "    return TP, TN, FP, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy\n",
    "def calc_accuracy(TP, TN, FP, FN):\n",
    "    acc = (TP+TN)/(TP+TN+FP+FN)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision\n",
    "def calc_precision(TP, FP):\n",
    "    prec = TP/(TP+FP)\n",
    "    return prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recall\n",
    "def calc_recall(TP, FN):\n",
    "    rec = TP/(TP+FN)\n",
    "    return rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#F1 score\n",
    "def f1_score(prec, rec):\n",
    "    f1 = (2*prec*rec)/(prec+rec)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель могла переобучиться, поскольку мы не делали валидацию (на отложенной выборке либо кроссвалидацию)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6. Реализация с L1 и L2 регуляризацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_logit(X, y, iterations=None, alpha=None, reg=0, lambda_=None):\n",
    "    assert reg == 'l2', 'Допустимые значения reg равны l2, l1 либо 0'\n",
    "    assert reg == 'l1', 'Допустимые значения reg равны l2, l1 либо 0'\n",
    "    assert reg == 0, 'Допустимые значения reg равны l2, l1 либо 0'\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    W = np.random.randn(X.shape[0])\n",
    "    n = X.shape[1]\n",
    "    \n",
    "    for i in range(1, iterations+1):\n",
    "        z = np.dot(W, X)\n",
    "        y_pred = sigmoid(z)\n",
    "        err = calc_logloss(y, y_pred)\n",
    "        \n",
    "        if reg == 'l2':\n",
    "            W -= alpha * ((1/n * np.dot((y_pred - y), X.T)) +  2 * lambda_ * W)\n",
    "        if reg == 'l1':\n",
    "            W -= alpha * ((1/n * np.dot((y_pred - y), X.T)) + 2 * ((lambda_* W)/np.abs(W)))\n",
    "        if reg == 0:\n",
    "            W -= alpha * (1/n * np.dot((y_pred - y), X.T))\n",
    "    \n",
    "    if i % (iterations / 10) == 0:\n",
    "        print(i, W, err)\n",
    "    \n",
    "    return W"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
